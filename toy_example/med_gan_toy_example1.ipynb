{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/han/anaconda/lib/python3.6/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_logit [None, 32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "def gaussians_8mode(sample_size):\n",
    "    scale = 2.\n",
    "    centers = [\n",
    "        (1,0),\n",
    "        (-1,0),\n",
    "        (0,1),\n",
    "        (0,-1),\n",
    "        (1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "        (1./np.sqrt(2), -1./np.sqrt(2)),\n",
    "        (-1./np.sqrt(2), 1./np.sqrt(2)),\n",
    "        (-1./np.sqrt(2), -1./np.sqrt(2))\n",
    "    ]\n",
    "    centers = [(scale*x,scale*y) for x,y in centers]\n",
    "    dataset = []\n",
    "    for i in range(sample_size):\n",
    "        point = np.random.randn(2)*.2\n",
    "        center = random.choice(centers)\n",
    "        point[0] += center[0]\n",
    "        point[1] += center[1]\n",
    "        dataset.append(point)\n",
    "    dataset = np.array(dataset, dtype='float32')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model      = 'dan_energy_batch' # One of: [dan_s, dan_energy_batch]\n",
    "batch_size = 512\n",
    "Z_dim      = 256\n",
    "s_lambda =1\n",
    "s_iter = 100\n",
    "\n",
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "    G_h3 = tf.nn.relu(tf.matmul(G_h2, G_W3) + G_b3)\n",
    "    G_logit = tf.matmul(G_h3, G_W4) + G_b4\n",
    "    return G_logit\n",
    "\n",
    "def gan_discriminator(x_real, x_fake):\n",
    "    \"\"\" I.e. Regular GAN discriminator \"\"\"\n",
    "    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)\n",
    "    D_h2_real = tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2)\n",
    "    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)\n",
    "    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4\n",
    "    D_prob_real = tf.nn.sigmoid(D_logit_real)\n",
    "\n",
    "    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)\n",
    "    D_h2_fake = tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2)\n",
    "    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)\n",
    "    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4\n",
    "    D_prob_fake = tf.nn.sigmoid(D_logit_fake)    \n",
    "    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake\n",
    "\n",
    "\n",
    "def gan_energy_discriminator(x_real1, x_real2, x_fake1, x_fake2):\n",
    "    \"\"\" I.e. Regular GAN discriminator \"\"\"\n",
    "    D_h1_real1 = tf.nn.relu(tf.matmul(x_real1 / 4, D_W1) + D_b1)\n",
    "    D_h2_real1 = tf.nn.relu(tf.matmul(D_h1_real1, D_W2) + D_b2)\n",
    "    D_h3_real1 = tf.nn.relu(tf.matmul(D_h2_real1, D_W3) + D_b3)\n",
    "    D_logit_real1 = tf.matmul(D_h3_real1, D_W4) + D_b4\n",
    "    # D_logit_real1 = tf.nn.l2_normalize(D_logit_real1, dim=1)\n",
    "    \n",
    "    \n",
    "    D_h1_real2 = tf.nn.relu(tf.matmul(x_real2 / 4, D_W1) + D_b1)\n",
    "    D_h2_real2 = tf.nn.relu(tf.matmul(D_h1_real2, D_W2) + D_b2)\n",
    "    D_h3_real2 = tf.nn.relu(tf.matmul(D_h2_real2, D_W3) + D_b3)\n",
    "    D_logit_real2 = tf.matmul(D_h3_real2, D_W4) + D_b4\n",
    "    # D_logit_real2 = tf.nn.l2_normalize(D_logit_real2, dim=1)\n",
    "\n",
    "\n",
    "    D_h1_fake1 = tf.nn.relu(tf.matmul(x_fake1 / 4, D_W1) + D_b1)\n",
    "    D_h2_fake1 = tf.nn.relu(tf.matmul(D_h1_fake1, D_W2) + D_b2)\n",
    "    D_h3_fake1 = tf.nn.relu(tf.matmul(D_h2_fake1, D_W3) + D_b3)\n",
    "    D_logit_fake1 = tf.matmul(D_h3_fake1, D_W4) + D_b4\n",
    "    # D_logit_fake1 = tf.nn.l2_normalize(D_logit_fake1, dim=1)\n",
    "    \n",
    "    \n",
    "    D_h1_fake2 = tf.nn.relu(tf.matmul(x_fake2 / 4, D_W1) + D_b1)\n",
    "    D_h2_fake2 = tf.nn.relu(tf.matmul(D_h1_fake2, D_W2) + D_b2)\n",
    "    D_h3_fake2 = tf.nn.relu(tf.matmul(D_h2_fake2, D_W3) + D_b3)\n",
    "    D_logit_fake2 = tf.matmul(D_h3_fake2, D_W4) + D_b4\n",
    "    # D_logit_fake2 = tf.nn.l2_normalize(D_logit_fake2, dim=1)\n",
    "    \n",
    "    print('D_logit', D_logit_real1.get_shape().as_list())\n",
    "   \n",
    "    return D_logit_real1, D_logit_real2,  D_logit_fake1, D_logit_fake2\n",
    "\n",
    "\n",
    "def dan_s_discriminator(x_real, x_fake):\n",
    "    \"\"\" A.k.a Sample classifier for DAN_S\"\"\"\n",
    "    D_h1_real = tf.nn.relu(tf.matmul(x_real / 4, D_W1) + D_b1)\n",
    "    D_h2_real = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_real, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h3_real = tf.nn.relu(tf.matmul(D_h2_real, D_W3) + D_b3)\n",
    "    D_logit_real = tf.matmul(D_h3_real, D_W4) + D_b4\n",
    "    D_prob_real = tf.nn.sigmoid(D_logit_real)\n",
    "\n",
    "    D_h1_fake = tf.nn.relu(tf.matmul(x_fake / 4, D_W1) + D_b1)\n",
    "    D_h2_fake = tf.reduce_mean(tf.nn.relu(tf.matmul(D_h1_fake, D_W2) + D_b2), axis=0, keep_dims=True)\n",
    "    D_h3_fake = tf.nn.relu(tf.matmul(D_h2_fake, D_W3) + D_b3)\n",
    "    D_logit_fake = tf.matmul(D_h3_fake, D_W4) + D_b4\n",
    "    D_prob_fake = tf.nn.sigmoid(D_logit_fake)\n",
    "\n",
    "    return D_prob_real, D_logit_real, D_prob_fake, D_logit_fake\n",
    "                        \n",
    "\n",
    "def sample_Z(m, n):\n",
    "    return np.random.normal(size=[m, n])\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "# Set up weights\n",
    "D_W1 = tf.Variable(xavier_init([2, 32]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([32, 32]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "D_W3 = tf.Variable(xavier_init([32, 32]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape=[32]))\n",
    "\n",
    "if model in ['dan_energy_batch']:\n",
    "    D_W4 = tf.Variable(xavier_init([32, 32]))\n",
    "    D_b4 = tf.Variable(tf.zeros(shape=[32]))\n",
    "else:\n",
    "    D_W4 = tf.Variable(xavier_init([32, 1]))\n",
    "    D_b4 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3, D_W4, D_b4]\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([Z_dim, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([128, 128]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W3 = tf.Variable(xavier_init([128,128]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W4 = tf.Variable(xavier_init([128, 2]))\n",
    "G_b4 = tf.Variable(tf.zeros(shape=[2]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3, G_W4, G_b4]\n",
    "\n",
    "\n",
    "# Set up variables\n",
    "if model in ['gan', 'dan_s']:\n",
    "    # Only one sample in this case\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    Z = tf.placeholder(tf.float32, shape=[None, Z_dim])    \n",
    "    G_sample = generator(Z)    \n",
    "    if model == 'gan':\n",
    "        D_prob_real, D_logit_real, D_prob_fake, D_logit_fake = gan_discriminator(X, G_sample)    \n",
    "    else:\n",
    "        D_prob_real, D_logit_real, D_prob_fake, D_logit_fake = dan_s_discriminator(X, G_sample)    \n",
    "        \n",
    "else:\n",
    "    # One variable per sample\n",
    "    X_1 = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    X_2 = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "    Z_1 = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "    Z_2 = tf.placeholder(tf.float32, shape=[None, Z_dim])\n",
    "    G_sample_1 = generator(Z_1)\n",
    "    G_sample_2 = generator(Z_2)\n",
    "    if model =='dan_energy_batch':\n",
    "        D_logit_real1, D_logit_real2,  D_logit_fake1, D_logit_fake2 = gan_energy_discriminator(X_1, X_2, G_sample_1, G_sample_2)\n",
    " \n",
    "        \n",
    "# Set up losses\n",
    "if model in ['gan', 'dan_s']:\n",
    "    D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "    D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "else:\n",
    "    if model == 'dan_energy_batch':\n",
    "        batch_logit_real1 = tf.reduce_mean(D_logit_real1, axis=0)\n",
    "        batch_logit_real2 = tf.reduce_mean(D_logit_real2, axis=0)\n",
    "        batch_logit_fake1 = tf.reduce_mean(D_logit_fake1, axis=0)\n",
    "        batch_logit_fake2 = tf.reduce_mean(D_logit_fake2, axis=0)\n",
    "        D_logit_11 = tf.norm(batch_logit_real1 - batch_logit_real2)\n",
    "        D_logit_10 = tf.norm(batch_logit_real1 - batch_logit_fake2)\n",
    "        D_logit_01 = tf.norm(batch_logit_fake1 - batch_logit_real2)\n",
    "        D_logit_00 = tf.norm(batch_logit_fake1 - batch_logit_fake2)\n",
    "        D_loss = -(D_logit_01 + D_logit_10 - D_logit_00 - D_logit_11)\n",
    "        G_loss = D_logit_01 + D_logit_10 - D_logit_00\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "print('')\n",
    "# Set up solvers\n",
    "D_solver = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5).minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = gaussians_8mode(40000)\n",
    "data_size = len(data)\n",
    "data = np.concatenate((data, data[:batch_size,:]), axis=0)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('step3')\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "save_fig_path = 'out_' + model\n",
    "if not os.path.exists(save_fig_path):\n",
    "    os.makedirs(save_fig_path)\n",
    "\n",
    "i = 0\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(data[:1000,0], data[:1000,1], 'b.')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-4,4])\n",
    "axes.set_ylim([-4,4])\n",
    "plt.title('True data distribution')\n",
    "plt.savefig(save_fig_path + '/real.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "from IPython import display\n",
    "np_samples = []\n",
    "plot_every = 500\n",
    "plt.figure(figsize=(5,5))\n",
    "D_loss_curr = 0\n",
    "G_loss_curr = 0\n",
    "\n",
    "for it in range(40000):\n",
    "    start_idx = it*batch_size%data_size\n",
    "    X_mb = data[start_idx:start_idx+batch_size, :]\n",
    "    \n",
    "    \n",
    "    if it<15000:\n",
    "        if model in ['gan', 'dan_s']:\n",
    "            _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "            _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        elif model in ['dan_energy_batch', 'dan_2s']:\n",
    "            sample_size = int(batch_size/2)\n",
    "            if (it+1) % 1 == 0:\n",
    "                _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X_1: X_mb[:sample_size,:], \\\n",
    "                                X_2: X_mb[sample_size:,:], \\\n",
    "                                Z_1: sample_Z(sample_size, Z_dim), \\\n",
    "                                Z_2: sample_Z(sample_size, Z_dim)})\n",
    "            _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={ \\\n",
    "                            X_1: X_mb[:sample_size,:], \\\n",
    "                            X_2: X_mb[sample_size:,:], \\\n",
    "                            Z_1: sample_Z(sample_size, Z_dim), \\\n",
    "                            Z_2: sample_Z(sample_size, Z_dim)})\n",
    "    else:\n",
    "        if model in ['gan', 'dan_s']:\n",
    "            _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})\n",
    "        elif model in ['dan_energy_batch']:\n",
    "            sample_size = int(batch_size/2)\n",
    "            _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={ \\\n",
    "                            X_1: X_mb[:sample_size,:], \\\n",
    "                            X_2: X_mb[sample_size:,:], \\\n",
    "                            Z_1: sample_Z(sample_size, Z_dim), \\\n",
    "                            Z_2: sample_Z(sample_size, Z_dim)})\n",
    "\n",
    "\n",
    "\n",
    "    if (it+1) % plot_every == 0 or it==0:\n",
    "        if model in ['gan', 'dan_s']:\n",
    "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(1000, Z_dim)})\n",
    "        else:\n",
    "            samples = sess.run(G_sample_1, feed_dict={Z_1: sample_Z(1000, Z_dim)})\n",
    "\n",
    "        np_samples.append(samples)\n",
    "        plt.clf()\n",
    "        plt.plot(samples[:,0], samples[:,1], 'b.')\n",
    "        axes = plt.gca()\n",
    "        axes.set_xlim([-4,4])\n",
    "        axes.set_ylim([-4,4])\n",
    "        if model=='dan_wgan':\n",
    "            plt.title('Iter: {}, loss(D): {:2.5f}, loss(G):{:2.5f}, entropy:{:2.5f}'.format(it+1, D_loss_curr, G_loss_curr, entropy_np))\n",
    "        else:\n",
    "            plt.title('Iter: {}, loss(D): {:2.5f}I , loss(G):{:2.5f}'.format(it+1, D_loss_curr, G_loss_curr))\n",
    "        plt.savefig('out_' + model + '/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')        \n",
    "        display.display(plt.gcf()) \n",
    "        display.clear_output(wait=True)\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
